{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.{Graph, VertexRDD}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.util.GraphGenerators\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msparkSession\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4d43b888\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@1e21305c"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "import org.apache.spark.graphx.{Graph, VertexRDD}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "                    .jupyter()\n",
    "                    .master(\"local[*]\")\n",
    "                    .appName(\"notebook\")\n",
    "                    .getOrCreate()\n",
    "val sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[0] at parallelize at cmd1.sc:2\n",
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = ParallelCollectionRDD[1] at parallelize at cmd1.sc:6\n",
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5e97b350"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2_0\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m\n",
       "\u001b[36mres2_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count all users which are postdocs\n",
    "graph.vertices.filter { case (id, (name, pos)) => pos == \"postdoc\" }.count\n",
    "// Count all the edges where src > dst\n",
    "graph.edges.filter(e => e.srcId > e.dstId).count\n",
    "graph.edges.filter { case Edge(src, dst, prop) => src > dst }.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfacts\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[24] at map at cmd3.sc:1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use the triplets view to create an RDD of facts.\n",
    "val facts: RDD[String] = graph.triplets.map(triplet =>\n",
    "    triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\n",
    "facts.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,(istoica,prof))\n",
      "(3,(rxin,student))\n",
      "(5,(franklin,prof))\n",
      "(7,(jgonzal,postdoc))\n",
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@40103d2c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// The valid subgraph will disconnect users 4 and 5 by removing user 0\n",
    "validGraph.vertices.collect.foreach(println(_))\n",
    "validGraph.triplets.map(\n",
    "  triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    ").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@6926d1a4\n",
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@1e1d9ea1\n",
       "\u001b[36mvalidCCGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@11df10ee"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run Connected Components\n",
    "val ccGraph = graph.connectedComponents() // No longer contains missing field\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// Restrict the answer to the valid subgraph\n",
    "val validCCGraph = ccGraph.mask(validGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moutDegrees\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mInt\u001b[39m] = VertexRDDImpl[97] at RDD at VertexRDD.scala:57\n",
       "\u001b[36mdegreeGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@45d17877"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outDegrees: VertexRDD[Int] = graph.outDegrees\n",
    "val degreeGraph = graph.outerJoinVertices(outDegrees) { (id, oldAttr, outDegOpt) =>\n",
    "  outDegOpt match {\n",
    "    case Some(outDeg) => outDeg\n",
    "    case None => 0 // No outDegree means zero outDegree\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,99.0)\n",
      "(56,76.0)\n",
      "(16,60.48148148148148)\n",
      "(80,92.5)\n",
      "(48,70.38888888888889)\n",
      "(32,56.666666666666664)\n",
      "(0,48.42307692307692)\n",
      "(24,55.7)\n",
      "(64,84.6)\n",
      "(40,71.65217391304348)\n",
      "(72,89.0)\n",
      "(8,50.51724137931034)\n",
      "(88,93.33333333333333)\n",
      "(41,67.76470588235294)\n",
      "(81,91.5)\n",
      "(25,67.66666666666667)\n",
      "(65,76.625)\n",
      "(73,78.71428571428571)\n",
      "(57,79.27272727272727)\n",
      "(33,57.294117647058826)\n",
      "(1,46.30434782608695)\n",
      "(89,96.2)\n",
      "(17,55.72727272727273)\n",
      "(9,56.0)\n",
      "(49,71.92307692307692)\n",
      "(34,60.65384615384615)\n",
      "(82,92.6)\n",
      "(66,84.125)\n",
      "(98,99.0)\n",
      "(50,79.17647058823529)\n",
      "(42,66.04761904761905)\n",
      "(74,91.125)\n",
      "(90,96.25)\n",
      "(18,59.08)\n",
      "(58,76.83333333333333)\n",
      "(26,65.34782608695652)\n",
      "(10,55.916666666666664)\n",
      "(2,55.18518518518518)\n",
      "(19,65.34782608695652)\n",
      "(59,77.28571428571429)\n",
      "(11,51.19047619047619)\n",
      "(35,64.9375)\n",
      "(27,60.51851851851852)\n",
      "(75,82.57142857142857)\n",
      "(51,64.76923076923077)\n",
      "(83,94.33333333333333)\n",
      "(67,82.375)\n",
      "(3,53.04545454545455)\n",
      "(91,93.8)\n",
      "(43,71.0625)\n",
      "(84,92.25)\n",
      "(52,69.92857142857143)\n",
      "(4,42.035714285714285)\n",
      "(76,90.875)\n",
      "(28,58.04545454545455)\n",
      "(36,71.08)\n",
      "(20,53.22222222222222)\n",
      "(12,49.8)\n",
      "(60,76.81818181818181)\n",
      "(44,68.41666666666667)\n",
      "(68,86.55555555555556)\n",
      "(13,53.7027027027027)\n",
      "(61,89.83333333333333)\n",
      "(21,57.45454545454545)\n",
      "(77,91.5)\n",
      "(53,78.76923076923077)\n",
      "(29,62.42857142857143)\n",
      "(93,99.0)\n",
      "(37,67.61538461538461)\n",
      "(45,75.57894736842105)\n",
      "(69,81.0)\n",
      "(85,96.0)\n",
      "(5,56.473684210526315)\n",
      "(22,60.892857142857146)\n",
      "(54,74.78571428571429)\n",
      "(46,60.77777777777778)\n",
      "(30,67.61111111111111)\n",
      "(14,52.2962962962963)\n",
      "(62,81.53333333333333)\n",
      "(6,54.43478260869565)\n",
      "(70,86.3)\n",
      "(38,67.27272727272727)\n",
      "(86,90.11111111111111)\n",
      "(78,93.5)\n",
      "(94,95.5)\n",
      "(39,65.5)\n",
      "(15,57.95454545454545)\n",
      "(47,63.55555555555556)\n",
      "(71,87.5)\n",
      "(55,77.41176470588235)\n",
      "(79,90.0)\n",
      "(23,51.0)\n",
      "(63,83.38461538461539)\n",
      "(7,59.83870967741935)\n",
      "(31,66.54545454545455)\n",
      "(87,93.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@62d164c\n",
       "\u001b[36molderFollowers\u001b[39m: \u001b[32mVertexRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m)] = VertexRDDImpl[128] at RDD at VertexRDD.scala:57\n",
       "\u001b[36mavgAgeOfOlderFollowers\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = VertexRDDImpl[130] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a graph with \"age\" as the vertex property.\n",
    "// Here we use a random graph for simplicity.\n",
    "val graph: Graph[Double, Int] =\n",
    "  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toDouble )\n",
    "// Compute the number of older followers and their total age\n",
    "val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](\n",
    "  triplet => { // Map Function\n",
    "    if (triplet.srcAttr > triplet.dstAttr) {\n",
    "      // Send message to destination vertex containing counter and age\n",
    "      triplet.sendToDst(1, triplet.srcAttr)\n",
    "    }\n",
    "  },\n",
    "  // Add counter and age\n",
    "  (a, b) => (a._1 + b._1, a._2 + b._2) // Reduce Function\n",
    ")\n",
    "// Divide total age by number of older followers to get average age of older followers\n",
    "val avgAgeOfOlderFollowers: VertexRDD[Double] =\n",
    "  olderFollowers.mapValues( (id, value) =>\n",
    "    value match { case (count, totalAge) => totalAge / count } )\n",
    "// Display the results\n",
    "avgAgeOfOlderFollowers.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmax\u001b[39m\n",
       "\u001b[36mmaxInDegree\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m27L\u001b[39m, \u001b[32m43\u001b[39m)\n",
       "\u001b[36mmaxOutDegree\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m31L\u001b[39m, \u001b[32m54\u001b[39m)\n",
       "\u001b[36mmaxDegrees\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m94L\u001b[39m, \u001b[32m88\u001b[39m)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a reduce operation to compute the highest degree vertex\n",
    "def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {\n",
    "  if (a._2 > b._2) a else b\n",
    "}\n",
    "// Compute the max degrees\n",
    "val maxInDegree: (VertexId, Int)  = graph.inDegrees.reduce(max)\n",
    "val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max)\n",
    "val maxDegrees: (VertexId, Int)   = graph.degrees.reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,0.15)\n",
      "(matei_zaharia,0.7013599933629602)\n",
      "(ladygaga,1.390049198216498)\n",
      "(BarackObama,1.4588814096664682)\n",
      "(jeresig,0.9993442038507723)\n",
      "(odersky,1.2973176314422592)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@18e81e30\n",
       "\u001b[36mranks\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = VertexRDDImpl[1040] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[1045] at map at cmd9.sc:5\n",
       "\u001b[36mranksByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = MapPartitionsRDD[1049] at map at cmd9.sc:9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the edges as a graph\n",
    "val graph = GraphLoader.edgeListFile(sc, \"data/graphx/followers.txt\")\n",
    "// Run PageRank\n",
    "val ranks = graph.pageRank(0.0001).vertices\n",
    "// Join the ranks with the usernames\n",
    "val users = sc.textFile(\"data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val ranksByUsername = users.join(ranks).map {\n",
    "  case (id, (username, rank)) => (username, rank)\n",
    "}\n",
    "// Print the result\n",
    "println(ranksByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,1)\n",
      "(matei_zaharia,3)\n",
      "(ladygaga,1)\n",
      "(BarackObama,1)\n",
      "(jeresig,3)\n",
      "(odersky,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@70c81086\n",
       "\u001b[36mcc\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mVertexId\u001b[39m] = VertexRDDImpl[1083] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[1101] at map at cmd10.sc:5\n",
       "\u001b[36mccByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mVertexId\u001b[39m)] = MapPartitionsRDD[1105] at map at cmd10.sc:9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the graph as in the PageRank example\n",
    "val graph = GraphLoader.edgeListFile(sc, \"data/graphx/followers.txt\")\n",
    "// Find the connected components\n",
    "val cc = graph.connectedComponents().vertices\n",
    "// Join the connected components with the usernames\n",
    "val users = sc.textFile(\"data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val ccByUsername = users.join(cc).map {\n",
    "  case (id, (username, cc)) => (username, cc)\n",
    "}\n",
    "// Print the result\n",
    "println(ccByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(justinbieber,0)\n",
      "(matei_zaharia,1)\n",
      "(ladygaga,0)\n",
      "(BarackObama,0)\n",
      "(jeresig,1)\n",
      "(odersky,1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@3a634233\n",
       "\u001b[36mtriCounts\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mInt\u001b[39m] = VertexRDDImpl[1175] at RDD at VertexRDD.scala:57\n",
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mString\u001b[39m)] = MapPartitionsRDD[1180] at map at cmd11.sc:6\n",
       "\u001b[36mtriCountByUsername\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mInt\u001b[39m)] = MapPartitionsRDD[1184] at map at cmd11.sc:10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the edges in canonical order and partition the graph for triangle count\n",
    "val graph = GraphLoader.edgeListFile(sc, \"data/graphx/followers.txt\", true)\n",
    "  .partitionBy(PartitionStrategy.RandomVertexCut)\n",
    "// Find the triangle count for each vertex\n",
    "val triCounts = graph.triangleCount().vertices\n",
    "// Join the triangle counts with the usernames\n",
    "val users = sc.textFile(\"data/graphx/users.txt\").map { line =>\n",
    "  val fields = line.split(\",\")\n",
    "  (fields(0).toLong, fields(1))\n",
    "}\n",
    "val triCountByUsername = users.join(triCounts).map { case (id, (username, tc)) =>\n",
    "  (username, tc)\n",
    "}\n",
    "// Print the result\n",
    "println(triCountByUsername.collect().mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,(1.453834747463902,List(BarackObama, Barack Obama)))\n",
      "(2,(1.3857595353443166,List(ladygaga, Goddess of Love)))\n",
      "(7,(1.2892158818481694,List(odersky, Martin Odersky)))\n",
      "(3,(0.9936187772892124,List(jeresig, John Resig)))\n",
      "(6,(0.697916749785472,List(matei_zaharia, Matei Zaharia)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m])] = MapPartitionsRDD[1188] at map at cmd12.sc:2\n",
       "\u001b[36mfollowerGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@770788bf\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m], \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@4d3ed67e\n",
       "\u001b[36msubgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m], \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@8fffe7a\n",
       "\u001b[36mpagerankGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@5c8f6174\n",
       "\u001b[36muserInfoWithPageRank\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m]), \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@37edbb1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load my user data and parse into tuples of user id and attribute list\n",
    "val users = (sc.textFile(\"data/graphx/users.txt\")\n",
    "  .map(line => line.split(\",\")).map( parts => (parts.head.toLong, parts.tail) ))\n",
    "\n",
    "// Parse the edge data which is already in userId -> userId format\n",
    "val followerGraph = GraphLoader.edgeListFile(sc, \"data/graphx/followers.txt\")\n",
    "\n",
    "// Attach the user attributes\n",
    "val graph = followerGraph.outerJoinVertices(users) {\n",
    "  case (uid, deg, Some(attrList)) => attrList\n",
    "  // Some users may not have attributes so we set them as empty\n",
    "  case (uid, deg, None) => Array.empty[String]\n",
    "}\n",
    "\n",
    "// Restrict the graph to users with usernames and names\n",
    "val subgraph = graph.subgraph(vpred = (vid, attr) => attr.size == 2)\n",
    "\n",
    "// Compute the PageRank\n",
    "val pagerankGraph = subgraph.pageRank(0.001)\n",
    "\n",
    "// Get the attributes of the top pagerank users\n",
    "val userInfoWithPageRank = subgraph.outerJoinVertices(pagerankGraph.vertices) {\n",
    "  case (uid, attrList, Some(pr)) => (pr, attrList.toList)\n",
    "  case (uid, attrList, None) => (0.0, attrList.toList)\n",
    "}\n",
    "\n",
    "println(userInfoWithPageRank.vertices.top(5)(Ordering.by(_._2._1)).mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
