{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.{Graph, VertexRDD}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.graphx.util.GraphGenerators\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msparkSession\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@69e9d79b\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@153676b9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "import org.apache.spark.graphx._\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "import org.apache.spark.graphx.{Graph, VertexRDD}\n",
    "import org.apache.spark.graphx.util.GraphGenerators\n",
    "\n",
    "val sparkSession = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "                    .jupyter()\n",
    "                    .master(\"local[*]\")\n",
    "                    .appName(\"notebook\")\n",
    "                    .getOrCreate()\n",
    "val sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36musers\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mVertexId\u001b[39m, (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m))] = ParallelCollectionRDD[0] at parallelize at cmd1.sc:2\n",
       "\u001b[36mrelationships\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mEdge\u001b[39m[\u001b[32mString\u001b[39m]] = ParallelCollectionRDD[1] at parallelize at cmd1.sc:6\n",
       "\u001b[36mdefaultUser\u001b[39m: (\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m) = (\u001b[32m\"John Doe\"\u001b[39m, \u001b[32m\"Missing\"\u001b[39m)\n",
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@785656cc"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an RDD for the vertices\n",
    "val users: RDD[(VertexId, (String, String))] =\n",
    "  sc.parallelize(Array((3L, (\"rxin\", \"student\")), (7L, (\"jgonzal\", \"postdoc\")),\n",
    "                       (5L, (\"franklin\", \"prof\")), (2L, (\"istoica\", \"prof\"))))\n",
    "// Create an RDD for edges\n",
    "val relationships: RDD[Edge[String]] =\n",
    "  sc.parallelize(Array(Edge(3L, 7L, \"collab\"),    Edge(5L, 3L, \"advisor\"),\n",
    "                       Edge(2L, 5L, \"colleague\"), Edge(5L, 7L, \"pi\")))\n",
    "// Define a default user in case there are relationship with missing user\n",
    "val defaultUser = (\"John Doe\", \"Missing\")\n",
    "// Build the initial Graph\n",
    "val graph = Graph(users, relationships, defaultUser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres2_0\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m\n",
       "\u001b[36mres2_2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Count all users which are postdocs\n",
    "graph.vertices.filter { case (id, (name, pos)) => pos == \"postdoc\" }.count\n",
    "// Count all the edges where src > dst\n",
    "graph.edges.filter(e => e.srcId > e.dstId).count\n",
    "graph.edges.filter { case Edge(src, dst, prop) => src > dst }.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfacts\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[24] at map at cmd3.sc:1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use the triplets view to create an RDD of facts.\n",
    "val facts: RDD[String] = graph.triplets.map(triplet =>\n",
    "    triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\n",
    "facts.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,(istoica,prof))\n",
      "(3,(rxin,student))\n",
      "(5,(franklin,prof))\n",
      "(7,(jgonzal,postdoc))\n",
      "rxin is the collab of jgonzal\n",
      "franklin is the advisor of rxin\n",
      "istoica is the colleague of franklin\n",
      "franklin is the pi of jgonzal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@2e330047"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// The valid subgraph will disconnect users 4 and 5 by removing user 0\n",
    "validGraph.vertices.collect.foreach(println(_))\n",
    "validGraph.triplets.map(\n",
    "  triplet => triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1\n",
    ").collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mccGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@15184cc4\n",
       "\u001b[36mvalidGraph\u001b[39m: \u001b[32mGraph\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m), \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@40fc6bc2\n",
       "\u001b[36mvalidCCGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mVertexId\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@547e4cb8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Run Connected Components\n",
    "val ccGraph = graph.connectedComponents() // No longer contains missing field\n",
    "// Remove missing vertices as well as the edges to connected to them\n",
    "val validGraph = graph.subgraph(vpred = (id, attr) => attr._2 != \"Missing\")\n",
    "// Restrict the answer to the valid subgraph\n",
    "val validCCGraph = ccGraph.mask(validGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36moutDegrees\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mInt\u001b[39m] = VertexRDDImpl[97] at RDD at VertexRDD.scala:57\n",
       "\u001b[36mdegreeGraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@49d12b5c"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outDegrees: VertexRDD[Int] = graph.outDegrees\n",
    "val degreeGraph = graph.outerJoinVertices(outDegrees) { (id, oldAttr, outDegOpt) =>\n",
    "  outDegOpt match {\n",
    "    case Some(outDeg) => outDeg\n",
    "    case None => 0 // No outDegree means zero outDegree\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,97.0)\n",
      "(56,77.28571428571429)\n",
      "(16,62.04761904761905)\n",
      "(80,88.42857142857143)\n",
      "(48,73.33333333333333)\n",
      "(32,72.42857142857143)\n",
      "(0,48.333333333333336)\n",
      "(24,58.94736842105263)\n",
      "(64,82.2)\n",
      "(40,76.70588235294117)\n",
      "(72,86.6)\n",
      "(8,59.7)\n",
      "(88,94.5)\n",
      "(41,73.22222222222223)\n",
      "(81,88.4)\n",
      "(25,64.94117647058823)\n",
      "(65,75.5)\n",
      "(73,89.0)\n",
      "(57,76.5)\n",
      "(33,64.0909090909091)\n",
      "(1,45.029411764705884)\n",
      "(89,93.66666666666667)\n",
      "(17,61.5)\n",
      "(9,60.130434782608695)\n",
      "(49,80.46666666666667)\n",
      "(34,71.75)\n",
      "(82,94.33333333333333)\n",
      "(66,78.75)\n",
      "(50,72.23076923076923)\n",
      "(42,72.93333333333334)\n",
      "(74,85.71428571428571)\n",
      "(90,95.0)\n",
      "(18,52.05263157894737)\n",
      "(58,77.6)\n",
      "(26,57.294117647058826)\n",
      "(10,47.8)\n",
      "(2,48.61290322580645)\n",
      "(19,66.78571428571429)\n",
      "(59,72.5)\n",
      "(11,55.76923076923077)\n",
      "(35,67.63636363636364)\n",
      "(27,68.875)\n",
      "(75,88.66666666666667)\n",
      "(51,73.8125)\n",
      "(83,91.0)\n",
      "(67,77.55555555555556)\n",
      "(3,42.78260869565217)\n",
      "(43,72.5)\n",
      "(84,92.5)\n",
      "(52,71.3076923076923)\n",
      "(4,49.32142857142857)\n",
      "(76,92.0)\n",
      "(28,67.33333333333333)\n",
      "(36,73.84210526315789)\n",
      "(92,95.66666666666667)\n",
      "(20,56.55)\n",
      "(12,51.25)\n",
      "(60,80.11111111111111)\n",
      "(44,72.17647058823529)\n",
      "(68,77.6)\n",
      "(13,61.964285714285715)\n",
      "(61,76.44444444444444)\n",
      "(21,52.285714285714285)\n",
      "(77,89.66666666666667)\n",
      "(53,75.85)\n",
      "(29,56.541666666666664)\n",
      "(93,96.0)\n",
      "(37,67.88888888888889)\n",
      "(45,74.57142857142857)\n",
      "(69,77.0)\n",
      "(85,91.0)\n",
      "(5,56.30434782608695)\n",
      "(22,60.857142857142854)\n",
      "(54,79.75)\n",
      "(46,74.83333333333333)\n",
      "(30,65.27777777777777)\n",
      "(14,55.291666666666664)\n",
      "(62,84.0)\n",
      "(6,50.916666666666664)\n",
      "(70,83.44444444444444)\n",
      "(38,68.375)\n",
      "(86,92.28571428571429)\n",
      "(78,89.0)\n",
      "(94,96.5)\n",
      "(39,68.6)\n",
      "(15,58.35)\n",
      "(47,68.0)\n",
      "(71,79.33333333333333)\n",
      "(55,71.3076923076923)\n",
      "(95,99.0)\n",
      "(79,89.2)\n",
      "(23,62.35294117647059)\n",
      "(63,78.0)\n",
      "(7,47.666666666666664)\n",
      "(31,67.76923076923077)\n",
      "(87,93.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mgraph\u001b[39m: \u001b[32mGraph\u001b[39m[\u001b[32mDouble\u001b[39m, \u001b[32mInt\u001b[39m] = org.apache.spark.graphx.impl.GraphImpl@44b0354d\n",
       "\u001b[36molderFollowers\u001b[39m: \u001b[32mVertexRDD\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mDouble\u001b[39m)] = VertexRDDImpl[128] at RDD at VertexRDD.scala:57\n",
       "\u001b[36mavgAgeOfOlderFollowers\u001b[39m: \u001b[32mVertexRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = VertexRDDImpl[130] at RDD at VertexRDD.scala:57"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a graph with \"age\" as the vertex property.\n",
    "// Here we use a random graph for simplicity.\n",
    "val graph: Graph[Double, Int] =\n",
    "  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, _) => id.toDouble )\n",
    "// Compute the number of older followers and their total age\n",
    "val olderFollowers: VertexRDD[(Int, Double)] = graph.aggregateMessages[(Int, Double)](\n",
    "  triplet => { // Map Function\n",
    "    if (triplet.srcAttr > triplet.dstAttr) {\n",
    "      // Send message to destination vertex containing counter and age\n",
    "      triplet.sendToDst(1, triplet.srcAttr)\n",
    "    }\n",
    "  },\n",
    "  // Add counter and age\n",
    "  (a, b) => (a._1 + b._1, a._2 + b._2) // Reduce Function\n",
    ")\n",
    "// Divide total age by number of older followers to get average age of older followers\n",
    "val avgAgeOfOlderFollowers: VertexRDD[Double] =\n",
    "  olderFollowers.mapValues( (id, value) =>\n",
    "    value match { case (count, totalAge) => totalAge / count } )\n",
    "// Display the results\n",
    "avgAgeOfOlderFollowers.collect.foreach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmax\u001b[39m\n",
       "\u001b[36mmaxInDegree\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m10L\u001b[39m, \u001b[32m39\u001b[39m)\n",
       "\u001b[36mmaxOutDegree\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m30L\u001b[39m, \u001b[32m96\u001b[39m)\n",
       "\u001b[36mmaxDegrees\u001b[39m: (\u001b[32mVertexId\u001b[39m, \u001b[32mInt\u001b[39m) = (\u001b[32m30L\u001b[39m, \u001b[32m125\u001b[39m)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define a reduce operation to compute the highest degree vertex\n",
    "def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {\n",
    "  if (a._2 > b._2) a else b\n",
    "}\n",
    "// Compute the max degrees\n",
    "val maxInDegree: (VertexId, Int)  = graph.inDegrees.reduce(max)\n",
    "val maxOutDegree: (VertexId, Int) = graph.outDegrees.reduce(max)\n",
    "val maxDegrees: (VertexId, Int)   = graph.degrees.reduce(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
