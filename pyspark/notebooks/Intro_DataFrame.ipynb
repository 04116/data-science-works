{"cells":[{"cell_type":"markdown","source":["# Creating DataFrames with Python"],"metadata":{}},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n\nprint department1\nprint employee2\nprint departmentWithEmployees1.employees[0].email"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create the first DataFrame from a list of the rows.\ndepartmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Create a second DataFrame from a list of rows.\ndepartmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Working with DataFrames"],"metadata":{}},{"cell_type":"code","source":["# Union 2 DataFrames.\nunionDF = df1.unionAll(df2)\ndisplay(unionDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Write the Unioned DataFrame to a Parquet file.\n# Remove the file if it exists\ndbutils.fs.rm(\"/tmp/df-example.parquet\", True)\nunionDF.write.parquet(\"/tmp/df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Read a DataFrame from the Parquet file.\nparquetDF = sqlContext.read.parquet(\"/tmp/df-example.parquet\")\ndisplay(parquetDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Explode the employees column.\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\neDF = sqlContext.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n\nprint eDF.select(F.explode(eDF.intlist).alias(\"anInt\")).collect()\neDF.select(F.explode(eDF.mapfield).alias(\"key\", \"value\")).show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import explode\ndf = parquetDF.select(explode(\"employees\").alias(\"e\"))\nexplodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\ndisplay(explodeDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Use filter() to return only the rows that match the given predicate.\nfilterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\n# use | instead of or\nfilterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# The where() clause is equivalent to filter().\nwhereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Replace null values with -- using DataFrame Na functions.\nnonNullDF = explodeDF.fillna(\"--\")\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Retrieve only rows with missing firstName or lastName.\nfilterNullDF = explodeDF.filter((col(\"firstName\").isNull()) | (col(\"lastName\").isNull())).sort(\"email\")\ndisplay(filterNullDF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Example aggregations using agg() and countDistinct().\nfrom pyspark.sql.functions import countDistinct\ncountDistinctDF = explodeDF.select(\"firstName\", \"lastName\").groupBy(\"firstName\", \"lastName\").agg(countDistinct(\"firstName\"))\ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Compare the DataFrame and SQL Query Physical Plans (Hint: They should be the same.)\ncountDistinctDF.explain()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["explodeDF.registerTempTable(\"table_example\")\ncountDistinctDF_sql = sqlContext.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM table_example GROUP BY firstName, lastName\")\ncountDistinctDF_sql.explain()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# Sum up all the salaries\nsalarySumDF = explodeDF.agg({\"salary\": \"sum\"})\ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Print the summary statistics for the salaries.\nexplodeDF.describe(\"salary\").show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["display(explodeDF.select(\"salary\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# An example using Pandas & Matplotlib Integration\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.clf()\npdDF = nonNullDF.toPandas()\npdDF.plot(x=\"firstName\", y=\"salary\", kind=\"bar\", rot=45)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Cleanup: Remove the parquet file.\ndbutils.fs.rm(\"/tmp/df-example.parquet\", True)"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"Intro_DataFrame","notebookId":651954930651402},"nbformat":4,"nbformat_minor":0}
